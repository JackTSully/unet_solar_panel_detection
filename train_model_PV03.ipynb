{"cells":[{"cell_type":"markdown","metadata":{},"source":["# I. Explore Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["%%capture\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def show_images(images, titles=None):\n","    if not titles:\n","        titles = [img.shape for img in images]\n","    fig, axes = plt.subplots(nrows=1, ncols=len(images), figsize=(10, 30))\n","    for i, ax in enumerate(axes):\n","        ax.imshow(images[i], cmap=\"summer\")\n","        ax.set_title(titles[i])\n","        ax.axis(\"off\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The dataset is divided into several directories based on the location the image is taken, following code will show the number of all images in each directory."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["root_path = \"/kaggle/input/solar-panel-detection-and-identification/PV03\" \n","categories_paths = os.listdir(root_path)\n","categories_paths = [os.path.join(root_path, cat_path) for cat_path in categories_paths]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for cat_path in categories_paths:\n","    for _, _, files in os.walk(cat_path):\n","        print(\"{}: {}\".format(cat_path, len(files)))"]},{"cell_type":"markdown","metadata":{},"source":["Example of image and its mask."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# Edit path to image and mask to see\n","image_path = '/kaggle/input/solar-panel-detection-and-identification/PV03/PV03_Rooftop/PV03_314602_1199205.bmp'\n","mask_path = '/kaggle/input/solar-panel-detection-and-identification/PV03/PV03_Rooftop/PV03_314602_1199205_label.bmp'\n","image = plt.imread(image_path)\n","mask = np.expand_dims(plt.imread(mask_path), axis=(-1))\n","image_shape = image.shape\n","mask_shape = mask.shape\n","\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 20))\n","\n","axes[0].imshow(image)\n","axes[0].set_title('Shape: ' + str(image_shape))\n","\n","axes[1].imshow(mask, cmap=\"summer\")\n","axes[1].set_title('Shape: ' + str(mask_shape))\n","\n","[ax.axis(\"off\") for ax in axes]\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["All image are 1024x1024 pixels which will be huge for our model. The masks are in binary format, 1 for solar panels and 0 for other things.\n","So we need to take this considerations:\n","* Down-sampling images for ease of training.\n","* We need just one neuron in the output layer of our model which tells probabilty of being solar panel.\n","* Due to high resolution of the images, we cannot fit all of the dataset in RAM so creating the dataset from a generator is necessary."]},{"cell_type":"markdown","metadata":{},"source":["# II. Dataset Preparation"]},{"cell_type":"markdown","metadata":{},"source":["In this part, we should take care of several things:\n","* Creating a generator to read images from memory on the fly (during training).\n","* Spliting the dataset into train and test for further evaluation.\n","* Creating two datasets from the generators"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images_paths = []\n","for cat_path in categories_paths:\n","    for root, _, files in os.walk(cat_path):\n","        cd_images = [os.path.join(root, file) for file in files]\n","        [images_paths.append(img) for img in cd_images]\n","images_paths = sorted(images_paths)\n","images_paths[:6]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(images_paths)"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the images are in the odd indices and their mask is in the next index, so knowing this structure, we will divide the indices to `train_idx` and `test_idx` and then create these generators:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_images = len(images_paths)\n","#new_size = (256, 256)\n","new_size = (512, 512)\n","images_idx = range(0, n_images, 2)\n","train_idx, test_idx = train_test_split(images_idx, test_size=0.15)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["total_test=0\n","total=0\n","for i in test_idx:\n","    if 'Cropland' in images_paths[i]:\n","        total+=1\n","print('Ground Cropland:',total)\n","total_test=total_test+total\n","\n","total=0\n","for i in test_idx:\n","    if 'Grassland' in images_paths[i]:\n","        total+=1\n","print('Ground Grassland:',total)\n","total_test=total_test+total\n","\n","total=0\n","for i in test_idx:\n","    if 'SalineAlkali' in images_paths[i]:\n","        total+=1\n","print('Ground SalineAlkali:',total)\n","total_test=total_test+total\n","\n","total=0\n","for i in test_idx:\n","    if 'Shrubwood' in images_paths[i]:\n","        total+=1\n","print('Ground Shrubwood:',total)\n","total_test=total_test+total\n","\n","total=0\n","for i in test_idx:\n","    if 'WaterSurface' in images_paths[i]:\n","        total+=1\n","print('Ground WaterSurface:',total)\n","total_test=total_test+total\n","\n","total=0\n","for i in test_idx:\n","    if 'Rooftop' in images_paths[i]:\n","        total+=1\n","print('Rooftop:',total)\n","total_test=total_test+total\n","\n","print('Total used for testing:', total_test)"]},{"cell_type":"markdown","metadata":{},"source":["Downsampling the PV03 images so they are more similar to satellite images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torchvision.transforms as transforms \n","\n","def downsample_image(input_path, scale_factor):\n","    # Open the image\n","    img = Image.open(input_path)\n","\n","    # Get the original resolution\n","    original_resolution = img.size\n","\n","    # Calculate the new size based on the scale factor\n","    new_resolution = tuple(int(dim * scale_factor) for dim in original_resolution)\n","\n","    # Resize the image while maintaining resolution\n","    resized_img = img.resize(new_resolution, Image.LANCZOS)\n","\n","    upscaled_img = resized_img.resize(original_resolution, Image.LANCZOS)\n","\n","    \n","    \n","    img_numpy = np.array(upscaled_img)\n","    \n","    return img_numpy"]},{"cell_type":"markdown","metadata":{},"source":["Example of downsampled image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_path = '/kaggle/input/solar-panel-detection-and-identification/PV03/PV03_Rooftop/PV03_314602_1199205.bmp'\n","\n","og = plt.imread(image_path)\n","\n","downsamp = downsample_image(image_path,0.375)\n","\n","\n","show_images((og,downsamp),['Original', 'Down sampled'])"]},{"cell_type":"markdown","metadata":{},"source":["Create generator for train and test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_dataset_generator():\n","    for i in train_idx:\n","        image = (\n","            tf.convert_to_tensor(downsample_image(images_paths[i],0.375), dtype=tf.float32)\n","        )\n","        mask = (\n","            tf.convert_to_tensor(\n","                np.expand_dims(plt.imread(images_paths[i + 1]), axis=(-1)),\n","                dtype=tf.float32,\n","            )\n","            / 255.0\n","        )\n","\n","        image = tf.image.resize(image, new_size)\n","        mask = tf.image.resize(mask, new_size)\n","\n","        yield image, mask\n","        \n","\n","def test_dataset_generator():\n","    for i in test_idx:\n","        image = (\n","            tf.convert_to_tensor(downsample_image(images_paths[i],0.375), dtype=tf.float32)\n","        )\n","        mask = (\n","            tf.convert_to_tensor(\n","                np.expand_dims(plt.imread(images_paths[i + 1]), axis=(-1)),\n","                dtype=tf.float32,\n","            )\n","            / 255.0\n","        )\n","\n","        image = tf.image.resize(image, new_size)\n","        mask = tf.image.resize(mask, new_size)\n","\n","        yield image, mask"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-05-19T11:52:07.572859Z","iopub.status.busy":"2023-05-19T11:52:07.57237Z","iopub.status.idle":"2023-05-19T11:52:07.606633Z","shell.execute_reply":"2023-05-19T11:52:07.604918Z","shell.execute_reply.started":"2023-05-19T11:52:07.572821Z"}},"source":["Create tensorflow datasets:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_generator(\n","    train_dataset_generator,\n","    output_signature=(\n","        tf.TensorSpec(shape=(*new_size, 3), dtype=tf.float32),\n","        tf.TensorSpec(shape=(*new_size, 1), dtype=tf.float32),\n","    ),\n",")\n","\n","test_dataset = tf.data.Dataset.from_generator(\n","    test_dataset_generator,\n","    output_signature=(\n","        tf.TensorSpec(shape=(*new_size, 3), dtype=tf.float32),\n","        tf.TensorSpec(shape=(*new_size, 1), dtype=tf.float32),\n","    ),\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, we wil check if the dataset is working properly or not."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for item in train_dataset.shuffle(20).take(1):\n","    show_images((item[0]/255,item[1]))"]},{"cell_type":"markdown","metadata":{},"source":["# III. UNet Model"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:10:59.652877Z","iopub.status.busy":"2023-05-19T12:10:59.652308Z","iopub.status.idle":"2023-05-19T12:10:59.663864Z","shell.execute_reply":"2023-05-19T12:10:59.661589Z","shell.execute_reply.started":"2023-05-19T12:10:59.652815Z"}},"source":["We will be utilizing the segmentation-models library to obtain a Unet model with a inceptionresnetv2 backbone.\n","The Unet model is pretrained on the ImageNet dataset: https://www.image-net.org/"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","!pip install -U -q segmentation-models\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","\n","import segmentation_models as sm"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["load_model = False\n","backbone = 'inceptionresnetv2'\n","batch_size = 16"]},{"cell_type":"markdown","metadata":{},"source":["Before going into training we need to make sure:\n","* The data is preprocessed regard to the backbone model.\n","* The dataset is splitted into train and validation dataset.\n","* The encoder part of the UNet is freezed so the model can be trained within a reasonable time frame."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["backbone_preprocess = sm.get_preprocessing(backbone) \n","preprocess_fn = lambda x, y: (backbone_preprocess(x), y) # Converts RGB values into the range of -1 to 1 for images only (not mask)\n","\n","train_dataset = train_dataset.map(preprocess_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def is_test(x, _):\n","    return x % 4 == 0\n","\n","def is_train(x, y):\n","    return not is_test(x, y)\n","\n","\n","recover = lambda x, y: y\n","\n","valid_dataset = train_dataset.enumerate().filter(is_test).map(recover).batch(batch_size)\n","\n","train_dataset = train_dataset.enumerate().filter(is_train).map(recover).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = sm.Unet(\n","    backbone,\n","    classes=1,\n","    encoder_weights=\"imagenet\",\n","    encoder_freeze=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**Loss function used**: Diceloss <br> **Metrics used**: IOU Score, Binary Accuracy, Precision, and Recall"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loss = sm.losses.DiceLoss()\n","metrics = [sm.metrics.IOUScore(),'binary_accuracy',keras.metrics.Precision(),keras.metrics.Recall()]\n","model.compile(\"Adam\", loss=loss, metrics=metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#keras.utils.plot_model(model, show_shapes=True) # Uncomment for seeing the model graph"]},{"cell_type":"markdown","metadata":{},"source":["# IV. Training Model"]},{"cell_type":"markdown","metadata":{},"source":["In this part we will train the model.\n","First we define a callback for visualize the learning process with one instance of the training dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DisplayCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_begin(self, epoch, logs=None):\n","        if (epoch + 1) % 5 == 0:\n","            for item in train_dataset.unbatch().shuffle(1).take(1):\n","                image = item[0]\n","                mask_4d = self.model.predict(np.expand_dims(image, axis=(0)))\n","                mask = np.squeeze(mask_4d, axis=0)\n","                image_converted = (image + 1) / 2\n","                show_images((image_converted, mask))\n","\n","display_cb = DisplayCallback()\n","early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights='True')\n","checkpoint_cb = keras.callbacks.ModelCheckpoint('model_unet-inceptionresnetv2_cp.keras', verbose=1, save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for item in train_dataset.unbatch().shuffle(10).take(2):\n","    image = item[0]\n","    image_converted = (image + 1) / 2 # Converts RGB values back to the range of 0 to 1\n","    mask = item[1]\n","    show_images((image_converted, mask))"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:15:44.388177Z","iopub.status.busy":"2023-05-19T12:15:44.387638Z","iopub.status.idle":"2023-05-19T12:15:44.396726Z","shell.execute_reply":"2023-05-19T12:15:44.395196Z","shell.execute_reply.started":"2023-05-19T12:15:44.388138Z"}},"source":["The following code will train the model. In the case of training on the PV03 dataset it will take around 2 hours running on a P100 GPU with kaggle."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with tf.device(\"/device:GPU:0\"):\n","    history = model.fit(\n","        train_dataset,\n","        batch_size=batch_size,\n","        epochs=100,\n","        validation_data=valid_dataset,\n","        callbacks=[display_cb, early_stopping_cb, checkpoint_cb],\n","    )\n","history = pd.DataFrame.from_dict(history.history)\n","\n","history.to_csv(\"history_unet-inceptionresnetv2.csv\", index=False)\n","model.save(\"model_unet-inceptionresnetv2.keras\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's take a look on train and validation loss and IoU score:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history['loss'])\n","plt.plot(history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history['iou_score'])\n","plt.plot(history['val_iou_score'])\n","plt.title('model IoU score')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# V. Evaluate Model"]},{"cell_type":"markdown","metadata":{},"source":["We will evaluate the model. Doing this, we will take 10 instances from test set and see the result, note that the model has never seen these images."]},{"cell_type":"markdown","metadata":{},"source":["Apply backbone preprocessing to test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset = test_dataset.map(preprocess_fn)"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["threshold = 0.9\n","\n","for item in test_dataset.shuffle(100).take(10):\n","        image = item[0]\n","        true_mask = item[1]\n","        mask_4d = model.predict(np.expand_dims(image, axis=(0)))\n","        pred_mask_proba = np.squeeze(mask_4d, axis=0)\n","        pred_mask = np.where(pred_mask_proba > threshold, 1, 0)\n","        \n","        image_converted = (image + 1) / 2\n","        \n","        show_images(\n","            (image_converted, true_mask, pred_mask_proba, pred_mask), \n","            [\"Image\", \"True Mask\", \"Model Probability\", \"Model Prediciton @\"+str(threshold)]\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.evaluate(test_dataset.batch(batch_size))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2735184,"sourceId":4778133,"sourceType":"datasetVersion"},{"datasetId":4878637,"sourceId":8227466,"sourceType":"datasetVersion"},{"modelInstanceId":32875,"sourceId":38960,"sourceType":"modelInstanceVersion"},{"modelInstanceId":33428,"sourceId":39674,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30476,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
